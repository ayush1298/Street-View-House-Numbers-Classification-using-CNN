{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Street View House Numbers(SNVH) Classification using Convolutional Neural Network(CNN)","metadata":{}},{"cell_type":"markdown","source":"The Street View House Numbers (SVHN) dataset is one of the most popular benchmarks for object recognition tasks in academic papers. The images were obtained from house numbers in Google Street View images, are hosted by Stanford University and are very similar in philosophy with the MNIST dataset. However, the original purpose of this dataset is to solve a harder problem: that of recognizing digits and numbers in natural scene images.","metadata":{}},{"cell_type":"markdown","source":"The data of the Street View House Numbers dataset, which can originally be found [here](http://ufldl.stanford.edu/housenumbers) are originally in .mat, i.e. files which can be best processed with MATLAB; which are preprocessed before used. It is important to note that the data are divided into two formats and in this project we are going to use **Format 2**:\n\n- *Format 1*: The original, variable-resolution colored house-number images with character level bounding boxes.\n- *Format 2*: The cropped digits (32x32 pixels) which follow the philosophy of the MNIST dataset more closely, but also contain some distracting digits to the sides of the digit of interest.","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports","metadata":{}},{"cell_type":"code","source":"#Importing Libraries\nimport numpy as np\nimport keras\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy.io import loadmat\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import ImageDataGenerator\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random state\n\nnp.random.seed(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Loading and preprocessing","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"# Loading the trainingand testing data(in .mat format)\n\ntrain_raw = loadmat('../input/svhndataset/train_32x32.mat')\ntest_raw = loadmat('../input/svhndataset/test_32x32.mat')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load images and labels\n\ntrain_images = np.array(train_raw['X'])\ntest_images = np.array(test_raw['X'])\n\ntrain_labels = train_raw['y']\ntest_labels = test_raw['y']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the data\n\nprint(train_images.shape)\nprint(test_images.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix the axes of the images\n\ntrain_images = np.moveaxis(train_images, -1, 0)\ntest_images = np.moveaxis(test_images, -1, 0)\n\nprint(train_images.shape)\nprint(test_images.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each image is 32x32 pixels and it has 3 channels RGB. Training set contain 73257 imgaes while test set contain 26032 images.","metadata":{}},{"cell_type":"code","source":"# Plot a random image and its label\n\nplt.imshow(train_images[13529])\nplt.show()\n\nprint('Label: ', train_labels[13529])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert train and test images into 'float64' type\n\ntrain_images = train_images.astype('float64')\ntest_images = test_images.astype('float64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert train and test labels into 'int64' type\n\ntrain_labels = train_labels.astype('int64')\ntest_labels = test_labels.astype('int64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the images data\n\nprint('Min: {}, Max: {}'.format(train_images.min(), train_images.max()))\n\ntrain_images /= 255.0\ntest_images /= 255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encoding of train and test labels\n\nlb = LabelBinarizer()\ntrain_labels = lb.fit_transform(train_labels)\ntest_labels = lb.fit_transform(test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train data into train and validation sets with split of 85% training and 15% validaion dataset\n\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_labels,\n                                                  test_size=0.15, random_state=22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. CNN model","metadata":{}},{"cell_type":"markdown","source":"In order to get more acuarate results out of our model, we are going to use data augmentation where we randomly rotate images, zoom them in and out, shift them up and down. Bu not shift them horizontally, since there are also distracting digits in the images.","metadata":{}},{"cell_type":"code","source":"# Data augmentation\n\ndatagen = ImageDataGenerator(rotation_range=8,\n                             zoom_range=[0.95, 1.05],\n                             height_shift_range=0.10,\n                             shear_range=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to determine a good learning rate for the optimizer of our model (here, we use the AMSGrad variant of the Adam optimizer), we set a callback in an auxillary model which will gradually increase the learning rate of the optimizer.\nAMSGrad is an extension to the Adam version of gradient descent that attempts to improve the convergence properties of the algorithm, avoiding large abrupt changes in the learning rate for each input variable.","metadata":{}},{"cell_type":"markdown","source":"We are creating an auxillary model in order to find suitable value of learning rate that we will be usinf in our ADAM's optimizer.","metadata":{}},{"cell_type":"code","source":"# Define auxillary model\n\nkeras.backend.clear_session()\n\naux_model = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                           activation='relu',\n                           input_shape=(32, 32, 3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(64, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(128, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.4),    \n    keras.layers.Dense(10,  activation='softmax')\n])\n\nlr_schedule = keras.callbacks.LearningRateScheduler(\n              lambda epoch: 1e-4 * 10**(epoch / 10))\noptimizer = keras.optimizers.Adam(lr=1e-4, amsgrad=True)\naux_model.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                 metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model in order to determine best learning rate\n\nhistory = aux_model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n                              epochs=30, validation_data=(X_val, y_val),\n                              callbacks=[lr_schedule])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Increasing epochs will not always increase accuracy. Increasing epochs makes sense only when we have large dataset. But model will reach a point where increasing epochs will not improve accuracy. Decreasing accuracy as training progresses means that the learning rate of model is too high. Models weights are changing a lot due to the high learning rate and therefore moving away from the local minimum where accuracy would be at its highest.We can see that after 25 epochs suddenly the accuracy of the model decreases which means that we have to change learning rate or decrease the epochs.","metadata":{}},{"cell_type":"code","source":"# Plot Learning Rate vs. Loss\n\nplt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-4, 1e-1, 0, 4])\nplt.xlabel('Learning Rate')\nplt.ylabel('Training Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the loss follows a very specific trajectory: a rapid drop followed by a relatively flat line which shoots back up after a certain point. Thus, it is better to choose a learning rate in the region where the loss is stable; a reasonable choice would be **lr = 0.01** (or 1e-3).","metadata":{}},{"cell_type":"code","source":"# Define actual model\n\nkeras.backend.clear_session()\n\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                           activation='relu',\n                           input_shape=(32, 32, 3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(64, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(128, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.4),    \n    keras.layers.Dense(10,  activation='softmax')\n])\n\nearly_stopping = keras.callbacks.EarlyStopping(patience=8)\noptimizer = keras.optimizers.Adam(lr=1e-3, amsgrad=True)\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(\n                   '/kaggle/working/best_cnn.h5', \n                   save_best_only=True)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model in order to make predictions\n\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n                              epochs=70, validation_data=(X_val, y_val),\n                              callbacks=[early_stopping, model_checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate train and validation accuracies and losses\n\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize epochs vs. train and validation accuracies and losses\n\nplt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend()\nplt.title('Epochs vs. Training and Validation Accuracy')\n    \nplt.subplot(1, 2, 2)\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Epochs vs. Training and Validation Loss')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After seeing how the algorithm converged, we can now evaluate the model's performance on the test data.","metadata":{}},{"cell_type":"code","source":"# Evaluate model on test data\ntest_loss, test_acc = model.evaluate(x=test_images, y=test_labels, verbose=0)\n\nprint('Test accuracy is: {:0.4f} \\nTest loss is: {:0.4f}'.\n      format(test_acc, test_loss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This result implies that our error ranges from ~4% to ~4.4%. This error can be reduced by tuning hyperparameters that we use in our model and doing some other modifictions like:\n- Change the way the images are transformed in the augmentation process.\n- Change the architecture of our model by adding extra blocks, changing the kernel sizes, making it deeper, etc.\n- Train multiple CNNs and make ensemble predictions.\n- Use some of the extra data which can be found along with the original dataset. ","metadata":{}},{"cell_type":"markdown","source":"## 4. Visualizations and insights","metadata":{}},{"cell_type":"markdown","source":"Using Confusion Matrix and Feature Maps, we will make visualizations which make us better understandingof working of Convolutional Neural Network(CNN).\n\n- The **Confusion Matrix** of the model on the training data, so as to get a sense of how it performs on each class label and how the misclassifications are distributed.\n- The **Feature Maps** for a random input image, so as to get a sense of how our model learns the features in each convolutional layer.","metadata":{}},{"cell_type":"code","source":"# Get predictions and apply inverse transformation to the labels\n\ny_pred = model.predict(X_train)\n\ny_pred = lb.inverse_transform(y_pred, lb.classes_)\ny_train = lb.inverse_transform(y_train, lb.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the confusion matrix\n\nmatrix = confusion_matrix(y_train, y_pred, labels=lb.classes_)\n\nfig, ax = plt.subplots(figsize=(14, 12))\nsns.heatmap(matrix, annot=True, cmap='Greens', fmt='d', ax=ax)\nplt.title('Confusion Matrix for training dataset')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the training data consists mostly of 0s, 1s and 2s (in a descending order), while labels '5' up to '9' are underepresented. Furthermore, the confusion matrix can show us the particular problematic cases of our model.","metadata":{}},{"cell_type":"code","source":"# Ignore the errors in the plots\n\nnp.seterr(all='ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get convolutional layers\n\nlayers = [model.get_layer('conv2d_1'), \n          model.get_layer('conv2d_2'),\n          model.get_layer('conv2d_3'),\n          model.get_layer('conv2d_4'),\n          model.get_layer('conv2d_5'),\n          model.get_layer('conv2d_6')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a model which gives the outputs of the layers\n\nlayer_outputs = [layer.output for layer in layers]\nactivation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list with the names of the layers\n\nlayer_names = []\nfor layer in layers:\n    layer_names.append(layer.name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function which will plot the convolutional filters\n\ndef plot_convolutional_filters(img):\n    \n    img = np.expand_dims(img, axis=0)\n    activations = activation_model.predict(img)\n    images_per_row = 9\n    \n    for layer_name, layer_activation in zip(layer_names, activations): \n        n_features = layer_activation.shape[-1]\n        size = layer_activation.shape[1]\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n        for col in range(n_cols): \n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                 :, :,\n                                                 col * images_per_row + row]\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,\n                             row * size : (row + 1) * size] = channel_image\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='plasma')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = X_train[42500]\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_convolutional_filters(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above, we can see the main idea behind Convolutional Neural Networks, i.e. that as we go deeper into the layers, higher-level features are learned by our models. At first, we can easily understand what is going on but gradually, we find it difficult to keep up with the model's learning process. The first(initial) convolutional layers learn features such as edges and simple textures.Later convolutional layers learn features such as more complex textures and patterns.The last convolutional layers learn features such as objects or parts of objects.","metadata":{}},{"cell_type":"markdown","source":"## 5. Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this Project, we have trained a Convolutional Neural Network to recognize the digits in the Street View House Numbers dataset (Format 2). In particular, we have performed some minimal preprocessing of the data, we have augmented the data in various ways, we have created an auxillary model in order to find which learning rate we should choose for our optimizer and finally, we have trained the final CNN and evaluated it on the test images data. Furthermore, we have provided two useful visualizations (confusion matrix and feature maps) so as get a sense of how our model actually works and not view it as just a black-box process.We are able to get an accuracy of 94.91% in the model that we have train. Also we can use hyperparameters tuning and different architectures so as to improve the accuracy of the model.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}